{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5. 深度学习计算.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPGl7b2JDxlPMrVRjOdOKVc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 5.1. 层和块"],"metadata":{"id":"khOEnYJB6wWu"}},{"cell_type":"markdown","source":["从编程的角度来看，块由类（class）表示。它的任何子类都必须定义一个将其输入转换为输出的前向传播函数，并且必须存储任何必需的参数。注意，有些块不需要任何参数。"],"metadata":{"id":"pcnkusMq6-3y"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2D7Ovm4I6cPe","executionInfo":{"status":"ok","timestamp":1652217461712,"user_tz":240,"elapsed":160,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"42d49b04-5f10-4575-daec-9dcf16a55fd5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.0544,  0.0649, -0.0510, -0.0981,  0.0120, -0.2447, -0.1708, -0.1143,\n","         -0.0418,  0.1693],\n","        [ 0.0929,  0.0372,  0.0199, -0.1171, -0.0700, -0.1488, -0.1916, -0.1445,\n","         -0.1535,  0.1724]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":16}],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n","\n","X = torch.rand(2, 20)\n","net(X)"]},{"cell_type":"markdown","source":["在这个例子中，我们通过实例化nn.Sequential来构建我们的模型，层的执行顺序是作为参数传递的。简而言之，nn.Sequential定义了一种特殊的Module，即在PyTorch中表示一个块的类，它维护了一个由Module组成的有序列表。"],"metadata":{"id":"WruyTFNI7QD8"}},{"cell_type":"markdown","source":["## 5.1.1. 自定义块"],"metadata":{"id":"k2G70eVN7VDY"}},{"cell_type":"markdown","source":["在实现我们自定义块之前，我们简要总结一下每个块必须提供的基本功能：\n","\n","将输入数据作为其前向传播函数的参数。\n","\n","通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。\n","\n","计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。\n","\n","存储和访问前向传播计算所需的参数。\n","\n","根据需要初始化模型参数。"],"metadata":{"id":"h_s02O337Z_z"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    # 用模型参数声明层。这里，我们声明两个全连接的层\n","    def __init__(self):\n","        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n","        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n","        super().__init__()\n","        self.hidden = nn.Linear(20, 256)  # 隐藏层\n","        self.out = nn.Linear(256, 10)  # 输出层\n","\n","    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n","    def forward(self, X):\n","        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n","        return self.out(F.relu(self.hidden(X)))"],"metadata":{"id":"MGfUeeoU7GJT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["我们首先看一下前向传播函数，它以X作为输入，计算带有激活函数的隐藏表示，并输出其未规范化的输出值。在这个MLP实现中，两个层都是实例变量。要了解这为什么是合理的，可以想象实例化两个多层感知机（net1和net2），并根据不同的数据对它们进行训练。\n","\n","注意一些关键细节：首先，我们定制的__init__函数通过super().__init__()调用父类的__init__函数，省去了重复编写模版代码的痛苦。然后，我们实例化两个全连接层，分别为self.hidden和self.out。注意，除非我们实现一个新的运算符，否则我们不必担心反向传播函数或参数初始化，系统将自动生成这些。"],"metadata":{"id":"1382g_i58H4Z"}},{"cell_type":"code","source":["net = MLP()\n","net(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTGt8srS8HXd","executionInfo":{"status":"ok","timestamp":1652217461882,"user_tz":240,"elapsed":10,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"56ffab24-6b55-4e3d-920c-1143d797f0bf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1738,  0.1186,  0.1393, -0.2454, -0.0841,  0.0165, -0.1037,  0.2953,\n","          0.0709, -0.0179],\n","        [-0.0998,  0.0805,  0.0706, -0.2512,  0.0545,  0.0414, -0.0548,  0.2532,\n","          0.1171, -0.0531]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["## 5.1.2. 顺序块"],"metadata":{"id":"rlSJnvXx8kAt"}},{"cell_type":"markdown","source":["现在我们可以更仔细地看看Sequential类是如何工作的，回想一下Sequential的设计是为了把其他模块串起来。为了构建我们自己的简化的MySequential，我们只需要定义两个关键函数：\n","\n","一种将块逐个追加到列表中的函数。\n","\n","一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。"],"metadata":{"id":"zmFopvC49D9e"}},{"cell_type":"code","source":["class MySequential(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        for idx, module in enumerate(args):\n","            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n","            # 变量_modules中。module的类型是OrderedDict\n","            self._modules[str(idx)] = module\n","\n","    def forward(self, X):\n","        # OrderedDict保证了按照成员添加的顺序遍历它们\n","        for block in self._modules.values():\n","            X = block(X)\n","        return X"],"metadata":{"id":"0J55VCM18c7H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["__init__函数将每个模块逐个添加到有序字典_modules中。你可能会好奇为什么每个Module都有一个_modules属性？以及为什么我们使用它而不是自己定义一个Python列表？简而言之，_modules的主要优点是：在模块的参数初始化过程中，系统知道在_modules字典中查找需要初始化参数的子块。"],"metadata":{"id":"lOZ1sN34-AEN"}},{"cell_type":"code","source":["net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n","net(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWtPR0cX-CoM","executionInfo":{"status":"ok","timestamp":1652217461882,"user_tz":240,"elapsed":9,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"5eb4e866-da67-4c60-ba4f-cb5ccea40415"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0424, -0.0484,  0.0562,  0.0861, -0.1524, -0.0043,  0.1691,  0.1190,\n","         -0.1127,  0.2346],\n","        [-0.0622, -0.1979,  0.0331,  0.1863, -0.2566, -0.0619,  0.0577,  0.0687,\n","         -0.1372,  0.1148]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["## 5.1.3. 在前向传播函数中执行代码"],"metadata":{"id":"k8MTTy89A-8E"}},{"cell_type":"code","source":["class FixedHiddenMLP(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n","        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n","        self.linear = nn.Linear(20, 20)\n","\n","    def forward(self, X):\n","        X = self.linear(X)\n","        # 使用创建的常量参数以及relu和mm函数\n","        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n","        # 复用全连接层。这相当于两个全连接层共享参数\n","        X = self.linear(X)\n","        # 控制流\n","        while X.abs().sum() > 1:\n","            X /= 2\n","        return X.sum()"],"metadata":{"id":"2BdciZueA5Jt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = FixedHiddenMLP()\n","net(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DxX63H8zBB2U","executionInfo":{"status":"ok","timestamp":1652217461883,"user_tz":240,"elapsed":8,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"d3d59d55-ad1c-4d0a-8e2f-77335f4dbf59"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.2141, grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["class NestMLP(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n","                                 nn.Linear(64, 32), nn.ReLU())\n","        self.linear = nn.Linear(32, 16)\n","\n","    def forward(self, X):\n","        return self.linear(self.net(X))\n","\n","chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n","chimera(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxYuYcmOBIVj","executionInfo":{"status":"ok","timestamp":1652217461883,"user_tz":240,"elapsed":6,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"a01deaef-45c4-4ab3-fd77-c437b84253df"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.0469, grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["# 5.2. 参数管理"],"metadata":{"id":"VobwSFfnBPO0"}},{"cell_type":"markdown","source":["访问参数，用于调试、诊断和可视化。\n","\n","参数初始化。\n","\n","在不同模型组件间共享参数。"],"metadata":{"id":"wB0N9vIzK64o"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n","X = torch.rand(size=(2, 4))\n","net(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WvzbrEEmBKZX","executionInfo":{"status":"ok","timestamp":1652217462026,"user_tz":240,"elapsed":10,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"bc666b64-f51a-494b-d7c9-f0045275d3c0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1435],\n","        [0.1081]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["## 5.2.1. 参数访问"],"metadata":{"id":"pgar33ZxK_5v"}},{"cell_type":"code","source":["print(net[2].state_dict())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"de5LgmRRKFdL","executionInfo":{"status":"ok","timestamp":1652217462027,"user_tz":240,"elapsed":10,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"e2bffb4b-f017-4e83-d180-4d9c6475cea9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["OrderedDict([('weight', tensor([[ 0.1909, -0.1698,  0.2963, -0.0146,  0.2720, -0.0772,  0.0179,  0.1271]])), ('bias', tensor([0.1436]))])\n"]}]},{"cell_type":"markdown","source":["输出的结果告诉我们一些重要的事情：首先，这个全连接层包含两个参数，分别是该层的权重和偏置。两者都存储为单精度浮点数（float32）。注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。"],"metadata":{"id":"7ja_hYvzLTpm"}},{"cell_type":"markdown","source":["### 5.2.1.1. 目标参数"],"metadata":{"id":"lw9TO7keLYDz"}},{"cell_type":"markdown","source":["下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，提取后返回的是一个参数类实例，并进一步访问该参数的值。"],"metadata":{"id":"41a64aQFLwn_"}},{"cell_type":"code","source":["print(type(net[2].bias))\n","print(net[2].bias)\n","print(net[2].bias.data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXObdg6HLB3a","executionInfo":{"status":"ok","timestamp":1652217462027,"user_tz":240,"elapsed":9,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"c71d57d6-448c-4270-ee56-54b2f374db08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.nn.parameter.Parameter'>\n","Parameter containing:\n","tensor([0.1436], requires_grad=True)\n","tensor([0.1436])\n"]}]},{"cell_type":"markdown","source":["参数是复合的对象，包含值、梯度和额外信息。这就是我们需要显式参数值的原因。除了值之外，我们还可以访问每个参数的梯度。在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。"],"metadata":{"id":"dNlFG-UNL1aU"}},{"cell_type":"code","source":["net[2].weight.grad == None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DyYycpt8LaWT","executionInfo":{"status":"ok","timestamp":1652217462028,"user_tz":240,"elapsed":9,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"2ec5ea28-7c52-49df-9e4d-6bcf0b9bccc3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["### 5.2.1.2. 一次性访问所有参数"],"metadata":{"id":"BzTlevENL7Ei"}},{"cell_type":"code","source":["print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n","print(*[(name, param.shape) for name, param in net.named_parameters()])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pacAA608L5EO","executionInfo":{"status":"ok","timestamp":1652217462028,"user_tz":240,"elapsed":8,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"e5b35669-ee11-4dcd-8675-69bb2493b240"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n","('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"]}]},{"cell_type":"code","source":["net.state_dict()['2.bias'].data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQTaLZKjMAHi","executionInfo":{"status":"ok","timestamp":1652217462029,"user_tz":240,"elapsed":8,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"1fb35fa7-2eb4-4030-ee61-229567ba5074"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.1436])"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["### 5.2.1.3. 从嵌套块收集参数"],"metadata":{"id":"p4BWyLF8MHrS"}},{"cell_type":"code","source":["def block1():\n","    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n","                         nn.Linear(8, 4), nn.ReLU())\n","\n","def block2():\n","    net = nn.Sequential()\n","    for i in range(4):\n","        # 在这里嵌套\n","        net.add_module(f'block {i}', block1())\n","    return net\n","\n","rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n","rgnet(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFrTnJaxMGFc","executionInfo":{"status":"ok","timestamp":1652217462029,"user_tz":240,"elapsed":7,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"784a4bee-16fc-4adf-d99a-1bd177ea50e3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.0019],\n","        [0.0033]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["print(rgnet)"],"metadata":{"id":"YJu68gVDMMzj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652217470484,"user_tz":240,"elapsed":140,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"0f6dd294-851b-4e63-baeb-d868041a532b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Sequential(\n","    (block 0): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 1): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 2): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 3): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","  )\n","  (1): Linear(in_features=4, out_features=1, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["print(rgnet)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZFuasb-d1dxW","executionInfo":{"status":"ok","timestamp":1652217480028,"user_tz":240,"elapsed":146,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"595abda5-555b-4cb4-8de9-7144f77b1a9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Sequential(\n","    (block 0): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 1): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 2): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 3): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","  )\n","  (1): Linear(in_features=4, out_features=1, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["## 5.2.2. 参数初始化"],"metadata":{"id":"4YDoczJQ1iPH"}},{"cell_type":"markdown","source":["### 5.2.2.1. 内置初始化"],"metadata":{"id":"LsUGFRc21nNP"}},{"cell_type":"markdown","source":["让我们首先调用内置的初始化器。下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0。"],"metadata":{"id":"SPxVJX_PINMv"}},{"cell_type":"code","source":["def init_normal(m):\n","    if type(m) == nn.Linear:\n","        nn.init.normal_(m.weight, mean=0, std=0.01)\n","        nn.init.zeros_(m.bias)\n","net.apply(init_normal)\n","net[0].weight.data[0], net[0].bias.data[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VQkubN0h1gGt","executionInfo":{"status":"ok","timestamp":1652217518952,"user_tz":240,"elapsed":166,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"d75c27d7-b970-4aba-916a-0f8b143e9cbd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([-0.0019,  0.0167,  0.0070,  0.0100]), tensor(0.))"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["我们还可以将所有参数初始化为给定的常数，比如初始化为1。"],"metadata":{"id":"Mbta1MRAIVNp"}},{"cell_type":"code","source":["def init_constant(m):\n","    if type(m) == nn.Linear:\n","        nn.init.constant_(m.weight, 1)\n","        nn.init.zeros_(m.bias)\n","net.apply(init_constant)\n","net[0].weight.data[0], net[0].bias.data[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OaEHcMqn1pmy","executionInfo":{"status":"ok","timestamp":1652217553993,"user_tz":240,"elapsed":130,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"d81707c7-7d3b-4b45-8e3c-30bc6d5fe892"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1., 1., 1., 1.]), tensor(0.))"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["我们还可以对某些块应用不同的初始化方法。例如，下面我们使用Xavier初始化方法初始化第一个神经网络层，然后将第三个神经网络层初始化为常量值42。"],"metadata":{"id":"AiN5LHTNIYix"}},{"cell_type":"code","source":["def xavier(m):\n","    if type(m) == nn.Linear:\n","        nn.init.xavier_uniform_(m.weight)\n","def init_42(m):\n","    if type(m) == nn.Linear:\n","        nn.init.constant_(m.weight, 42)\n","\n","net[0].apply(xavier)\n","net[2].apply(init_42)\n","print(net[0].weight.data[0])\n","print(net[2].weight.data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lr8-ZRMc1yKq","executionInfo":{"status":"ok","timestamp":1652217560583,"user_tz":240,"elapsed":133,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"ccbd470a-c754-454f-99ff-cf3a0bfe13a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0.0108, -0.0590, -0.2855,  0.4665])\n","tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"]}]},{"cell_type":"markdown","source":["### 5.2.2.2. 自定义初始化"],"metadata":{"id":"ERYMzoR211v9"}},{"cell_type":"code","source":["def my_init(m):\n","    if type(m) == nn.Linear:\n","        print(\"Init\", *[(name, param.shape)\n","                        for name, param in m.named_parameters()][0])\n","        nn.init.uniform_(m.weight, -10, 10)\n","        m.weight.data *= m.weight.data.abs() >= 5\n","\n","net.apply(my_init)\n","net[0].weight[:2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lprP4mqp1zxQ","executionInfo":{"status":"ok","timestamp":1652217575850,"user_tz":240,"elapsed":152,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"b90024f3-9076-4ab4-a2ec-302d1254a401"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Init weight torch.Size([8, 4])\n","Init weight torch.Size([1, 8])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0000,  5.6330,  7.8434, -9.3558],\n","        [-0.0000, -8.1764, -0.0000,  0.0000]], grad_fn=<SliceBackward0>)"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["net[0].weight.data[:] += 1\n","net[0].weight.data[0, 0] = 42\n","net[0].weight.data[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LySwV5-513fo","executionInfo":{"status":"ok","timestamp":1652217588330,"user_tz":240,"elapsed":164,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"e77be333-01ae-4e04-c382-9f9bea225bae"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([42.0000,  6.6330,  8.8434, -8.3558])"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["## 5.2.3. 参数绑定"],"metadata":{"id":"BOL8zVsK182q"}},{"cell_type":"code","source":["# 我们需要给共享层一个名称，以便可以引用它的参数\n","shared = nn.Linear(8, 8)\n","net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n","                    shared, nn.ReLU(),\n","                    shared, nn.ReLU(),\n","                    nn.Linear(8, 1))\n","net(X)\n","# 检查参数是否相同\n","print(net[2].weight.data[0] == net[4].weight.data[0])\n","net[2].weight.data[0, 0] = 100\n","# 确保它们实际上是同一个对象，而不只是有相同的值\n","print(net[2].weight.data[0] == net[4].weight.data[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pp6NfubJ16i1","executionInfo":{"status":"ok","timestamp":1652217605322,"user_tz":240,"elapsed":137,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"cde7b2f5-a570-4617-d0c0-7183ada101a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([True, True, True, True, True, True, True, True])\n","tensor([True, True, True, True, True, True, True, True])\n"]}]},{"cell_type":"markdown","source":["# 5.3. 延后初始化"],"metadata":{"id":"hM7wJ3jvzavi"}},{"cell_type":"markdown","source":["## 5.3.1. 实例化网络"],"metadata":{"id":"fIZXKZ2rzuDv"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","net = tf.keras.models.Sequential([\n","    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n","    tf.keras.layers.Dense(10),\n","])"],"metadata":{"id":"JRNw-iLD1-so"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[net.layers[i].get_weights() for i in range(len(net.layers))]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sZDi-Ct8zyLs","executionInfo":{"status":"ok","timestamp":1652300927021,"user_tz":240,"elapsed":23,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"01ab686a-de1e-49f7-a19f-3f218c51e308"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[], []]"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["X = tf.random.uniform((2, 20))\n","net(X)\n","[w.shape for w in net.get_weights()]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HOVbQ-nmz0ps","executionInfo":{"status":"ok","timestamp":1652300940696,"user_tz":240,"elapsed":425,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"f8a0688a-a365-421a-ef0a-195e24120b54"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(20, 256), (256,), (256, 10), (10,)]"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# 5.4. 自定义层"],"metadata":{"id":"bn0DXyLBz9_W"}},{"cell_type":"markdown","source":["## 5.4.1. 不带参数的层"],"metadata":{"id":"MVy5K3DA0BV0"}},{"cell_type":"markdown","source":["要构建它，我们只需继承基础层类并实现前向传播功能。"],"metadata":{"id":"O3y4MDduJOqx"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch import nn\n","\n","\n","class CenteredLayer(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, X):\n","        return X - X.mean()"],"metadata":{"id":"PJXsGolBz4E6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layer = CenteredLayer()\n","layer(torch.FloatTensor([1, 2, 3, 4, 5]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q8wmvt6D0GjQ","executionInfo":{"status":"ok","timestamp":1652301064136,"user_tz":240,"elapsed":170,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"006c5818-c0e0-4b79-a5ce-7b7d09644b9b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-2., -1.,  0.,  1.,  2.])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())"],"metadata":{"id":"WD2TQ7Mt0WSz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y = net(torch.rand(4, 8))\n","Y.mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MHLHVcIy0bIO","executionInfo":{"status":"ok","timestamp":1652301093991,"user_tz":240,"elapsed":272,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"6e048b09-ef39-4b40-a468-dac676248211"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.8626e-09, grad_fn=<MeanBackward0>)"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["## 5.4.2. 带参数的层"],"metadata":{"id":"phonqfRh0gk-"}},{"cell_type":"markdown","source":["现在，让我们实现自定义版本的全连接层。回想一下，该层需要两个参数，一个用于表示权重，另一个用于表示偏置项。在此实现中，我们使用修正线性单元作为激活函数。该层需要输入参数：in_units和units，分别表示输入数和输出数。"],"metadata":{"id":"WId2n6-7JWUD"}},{"cell_type":"code","source":["class MyLinear(nn.Module):\n","    def __init__(self, in_units, units):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.randn(in_units, units))\n","        self.bias = nn.Parameter(torch.randn(units,))\n","    def forward(self, X):\n","        linear = torch.matmul(X, self.weight.data) + self.bias.data\n","        return F.relu(linear)"],"metadata":{"id":"sgKormZM0dlf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["接下来，我们实例化MyLinear类并访问其模型参数。"],"metadata":{"id":"weqOGlupJalg"}},{"cell_type":"code","source":["linear = MyLinear(5, 3)\n","linear.weight"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9Dg1iUB0un1","executionInfo":{"status":"ok","timestamp":1652301172136,"user_tz":240,"elapsed":304,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"5fb56df4-2fe9-4aa5-be13-31f7168f0d96"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[-0.9746, -0.8712, -0.1042],\n","        [-0.8893,  0.1659, -1.6141],\n","        [ 0.0546,  0.1851,  2.1352],\n","        [ 0.8160,  1.8363, -1.6074],\n","        [ 1.6919,  1.5625, -1.9794]], requires_grad=True)"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["我们可以使用自定义层直接执行前向传播计算。"],"metadata":{"id":"F2cY33mpJcxD"}},{"cell_type":"code","source":["linear(torch.rand(2, 5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GH8CNkgc0wr2","executionInfo":{"status":"ok","timestamp":1652301180560,"user_tz":240,"elapsed":163,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"81d034de-7492-4e31-8941-bd81d26c0f9e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.1386, 0.0000, 0.0000],\n","        [3.0258, 0.1289, 0.0000]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["我们还可以使用自定义层构建模型，就像使用内置的全连接层一样使用自定义层。"],"metadata":{"id":"7boKQbjgJfLM"}},{"cell_type":"code","source":["net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n","net(torch.rand(2, 64))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XwGpE7wc0ywm","executionInfo":{"status":"ok","timestamp":1652301190903,"user_tz":240,"elapsed":273,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"4f794d20-aae9-4c9e-844f-eedca9f100fa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[16.5141],\n","        [14.1852]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["# 5.5. 读写文件"],"metadata":{"id":"Ae1YTZNZ05dB"}},{"cell_type":"markdown","source":["## 5.5.1. 加载和保存张量"],"metadata":{"id":"mOUQiiHP09X0"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","x = torch.arange(4)\n","torch.save(x, 'x-file')"],"metadata":{"id":"irthflOI01Qr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x2 = torch.load('x-file')\n","x2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e4pyUPeD1ASS","executionInfo":{"status":"ok","timestamp":1652301243927,"user_tz":240,"elapsed":190,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"04423489-7503-4c93-9aec-2641521e4e1c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2, 3])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["y = torch.zeros(4)\n","torch.save([x, y],'x-files')\n","x2, y2 = torch.load('x-files')\n","(x2, y2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LCMTalmT1CP0","executionInfo":{"status":"ok","timestamp":1652301257475,"user_tz":240,"elapsed":176,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"530d3730-aac1-40b7-9c23-b4d1e14eabff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["mydict = {'x': x, 'y': y}\n","torch.save(mydict, 'mydict')\n","mydict2 = torch.load('mydict')\n","mydict2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34pCv3Q_1FgT","executionInfo":{"status":"ok","timestamp":1652301268169,"user_tz":240,"elapsed":195,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"8f0bfeb0-e472-403e-cff6-934f844c0284"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## 5.5.2. 加载和保存模型参数"],"metadata":{"id":"IpZOtv6V1KtR"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.hidden = nn.Linear(20, 256)\n","        self.output = nn.Linear(256, 10)\n","\n","    def forward(self, x):\n","        return self.output(F.relu(self.hidden(x)))\n","\n","net = MLP()\n","X = torch.randn(size=(2, 20))\n","Y = net(X)"],"metadata":{"id":"scOiccvD1IKc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(net.state_dict(), 'mlp.params')"],"metadata":{"id":"U3e47kF_1RBU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clone = MLP()\n","clone.load_state_dict(torch.load('mlp.params'))\n","clone.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MC9tTfdU1UI9","executionInfo":{"status":"ok","timestamp":1652301324537,"user_tz":240,"elapsed":181,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"2e0b9720-0c69-42a7-efde-d2100f8ca19e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLP(\n","  (hidden): Linear(in_features=20, out_features=256, bias=True)\n","  (output): Linear(in_features=256, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["Y_clone = clone(X)\n","Y_clone == Y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7zJN54CX1V5I","executionInfo":{"status":"ok","timestamp":1652301332570,"user_tz":240,"elapsed":258,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"2971e9c3-09fb-435b-8f6a-1302e3508621"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[True, True, True, True, True, True, True, True, True, True],\n","        [True, True, True, True, True, True, True, True, True, True]])"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["# 5.6. GPU"],"metadata":{"id":"WLjGgxF11gSA"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QnlfRfnC1X3n","executionInfo":{"status":"ok","timestamp":1652301408530,"user_tz":240,"elapsed":23,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"9b5c5fde-2251-45e5-bb9c-60130f6895e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed May 11 20:36:48 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   50C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["## 5.6.1. 计算设备"],"metadata":{"id":"l96j93-P1tyG"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_HiPjqEB1lmI","executionInfo":{"status":"ok","timestamp":1652301438384,"user_tz":240,"elapsed":3261,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"1a949aae-1c80-4775-c7ee-e7be9158fca3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(device(type='cpu'), device(type='cuda'), device(type='cuda', index=1))"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["torch.cuda.device_count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITih348R1w1R","executionInfo":{"status":"ok","timestamp":1652301449154,"user_tz":240,"elapsed":531,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"01081131-6be8-47ac-e419-fa8448e04dae"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["def try_gpu(i=0):  \n","    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n","    if torch.cuda.device_count() >= i + 1:\n","        return torch.device(f'cuda:{i}')\n","    return torch.device('cpu')\n","\n","def try_all_gpus(): \n","    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]\"\"\"\n","    devices = [torch.device(f'cuda:{i}')\n","             for i in range(torch.cuda.device_count())]\n","    return devices if devices else [torch.device('cpu')]\n","\n","try_gpu(), try_gpu(10), try_all_gpus()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBn4kurq10L4","executionInfo":{"status":"ok","timestamp":1652301471270,"user_tz":240,"elapsed":333,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"d8445c46-e674-4b90-b830-fa4dda98fd5d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(device(type='cuda', index=0),\n"," device(type='cpu'),\n"," [device(type='cuda', index=0)])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["## 5.6.2. 张量与GPU"],"metadata":{"id":"LVwc6ZDj17-T"}},{"cell_type":"code","source":["x = torch.tensor([1, 2, 3])\n","x.device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtAbC7MT13MV","executionInfo":{"status":"ok","timestamp":1652301487013,"user_tz":240,"elapsed":344,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"a92ac2b7-c08b-4f1b-cbab-88ace9bbea59"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["### 5.6.2.1. 存储在GPU上"],"metadata":{"id":"j2FpifrC1_q0"}},{"cell_type":"code","source":["X = torch.ones(2, 3, device=try_gpu())\n","X"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wr2pDJZY19d8","executionInfo":{"status":"ok","timestamp":1652301513839,"user_tz":240,"elapsed":10559,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"9e729288-cc0a-4d61-e0cc-49c3c6afede1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]], device='cuda:0')"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["Y = torch.rand(2, 3, device=try_gpu(0))\n","Y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xK51g86v2BgL","executionInfo":{"status":"ok","timestamp":1652301519392,"user_tz":240,"elapsed":530,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"eb0533be-7eb7-45bc-de49-2ab586214243"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.9387, 0.5419, 0.5454],\n","        [0.9719, 0.1203, 0.3608]], device='cuda:0')"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["### 5.6.2.2. 复制"],"metadata":{"id":"fIYhwIes2IJy"}},{"cell_type":"code","source":["Z = X.cuda(0)\n","print(X)\n","print(Z)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NgTB7psP2FYE","executionInfo":{"status":"ok","timestamp":1652301551401,"user_tz":240,"elapsed":336,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"a09f3ba1-2a5d-4105-9792-febf7dfa942d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]], device='cuda:0')\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.]], device='cuda:0')\n"]}]},{"cell_type":"code","source":["Y + Z"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71a-hB8A2Lwh","executionInfo":{"status":"ok","timestamp":1652301558411,"user_tz":240,"elapsed":430,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"2c9503a7-f524-41f1-cd2a-87bcaccf114e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.9387, 1.5419, 1.5454],\n","        [1.9719, 1.1203, 1.3608]], device='cuda:0')"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["## 5.6.3. 神经网络与GPU"],"metadata":{"id":"E5Cg0Gzs2SIp"}},{"cell_type":"code","source":["net = nn.Sequential(nn.Linear(3, 1))\n","net = net.to(device=try_gpu())"],"metadata":{"id":"TJmMhB4-2O7O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wgzOo0XR2Tk6","executionInfo":{"status":"ok","timestamp":1652301584430,"user_tz":240,"elapsed":938,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"524de7e8-de74-4d5d-c089-f150ddf36362"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1558],\n","        [0.1558]], device='cuda:0', grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["net[0].weight.data.device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WGyyLwJb2VF4","executionInfo":{"status":"ok","timestamp":1652301589385,"user_tz":240,"elapsed":350,"user":{"displayName":"Shumeng Lin","userId":"11180216978862928857"}},"outputId":"6a0b6e67-4f5c-41f8-c9ca-ef0503c28ade"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":[""],"metadata":{"id":"Sz0yHEoI2Wel"},"execution_count":null,"outputs":[]}]}